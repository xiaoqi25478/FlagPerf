FLAGPERF_PATH: "/workspace/FlagPerf/base"
FLAGPERF_LOG_PATH: "result"
VENDOR: "cambricon"
FLAGPERF_LOG_LEVEL: "info"
# "BENCHMARK" means benchmarks(torch), "TOOLKIT" means toolkits
# benchmarks using container_main to launch "torchrun benchmarks/<case>/main.py", nnodes * nproc
# toolkits using container_main to launch bash toolkits/<case>/<vendor>/main.sh, nnodes.
# only in benchmarks, flagperf will automatically execute benchmakrs/<case>/<vendor>/requirements.txt and env.sh
# all resources to be used in toolkits/<case>/<vendor>/main.sh, should be under toolkits/<case>/<vendor>/
BENCHMARKS_OR_TOOLKITS: "BENCHMARK"
HOSTS: ["127.0.0.1"]
NPROC_PER_NODE: 8
SSH_PORT: "1234"
HOSTS_PORTS: ["1234"]
MASTER_PORT: "29501"
SHM_SIZE: "32G"
ACCE_CONTAINER_OPT: ""
# for nvidia, using " -- gpus all"
# for xxx, using
PIP_SOURCE: "https://mirror.baidu.com/pypi/simple"
CLEAR_CACHES: True
# for nvidia, using "CUDA_VISIBLE_DEVICES"
# for xxx, using
ACCE_VISIBLE_DEVICE_ENV_NAME: "MLU_VISIBLE_DEVICES"
CASES: 
    "computation-BF16:MLU": "pytorch_2.1"
    "computation-FP16:MLU": "pytorch_2.1"
    "computation-FP32:MLU": "pytorch_2.1"
    "computation-TF32:MLU": "pytorch_2.1"
    "interconnect-h2d:MLU": "pytorch_2.1"
    "computation-INT8:MLU": "pytorch_2.1"
    "interconnect-MPI_intraserver:MLU": "pytorch_2.1"
    "interconnect-P2P_intraserver:MLU": "pytorch_2.1"
    "main_memory-bandwidth:MLU": "pytorch_2.1"
    "main_memory-capacity:MLU": "pytorch_2.1"
# nvidia "computation-FP64": "pytorch_2.3"
# nvidia "computation-FP32": "pytorch_2.3"
# nvidia "computation-TF32": "pytorch_2.3"
# nvidia "computation-FP16": "pytorch_2.3"
# nvidia "computation-BF16": "pytorch_2.3"
# nvidia "computation-INT8": "pytorch_2.3"
# nvidia "main_memory-bandwidth": "pytorch_2.3"
# nvidia "main_memory-capacity": "pytorch_2.3"
# nvidia "interconnect-h2d": "pytorch_2.3"
# nvidia "interconnect-P2P_intraserver": "pyorch_2.3"
# nvidia "interconnect-MPI_intraserver": "pytorch_2.3"
# nvidia "interconnect-P2P_interserver": "pytorch_ssh"
# nvidia "interconnect-MPI_interserver": "pytorch_ssh"

# Fine-grained chip model configuration
# nvidia "computation-FP64:A100": "pytorch_2.3"
# nvidia "computation-FP32:A100": "pytorch_2.3"
# nvidia "computation-TF32:A100": "pytorch_2.3"
# nvidia "computation-FP16:A100": "pytorch_2.3"
# nvidia "computation-BF16:A100": "pytorch_2.3"
# nvidia "computation-INT8:A100": "pytorch_2.3"
# nvidia "main_memory-bandwidth:A100": "pytorch_2.3"
# nvidia "main_memory-capacity:A100": "pytorch_2.3"
# nvidia "interconnect-h2d:A100": "pytorch_2.3"
# nvidia "interconnect-MPI_intraserver:A100": "pytorch_2.3"
# nvidia "interconnect-P2P_intraserver:A100": "pyorch_2.3"
# nvidia "interconnect-P2P_interserver:A100": "pytorch_ssh"
# nvidia "interconnect-MPI_interserver:A100": "pytorch_ssh"
